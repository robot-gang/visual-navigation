<html>
<head>
  <!-- Metas -->
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/>
  <meta name="author" content="Son Tran"/>
  <meta name="description" content="Final Project EE106B, Spring 2020, UC Berkeley"/>
  <meta name="keywords" content="Robot, Robotics, Vision, Computer Vision, Control, MPC, Learning, Feedback Control"/>

  <!-- JavaScripts -->
  <!-- <script src="./index_files/jsapi"></script> -->
  <!-- <script>google.load("jquery", "1.3.2");</script> -->
  <script src="./js/humanav_demo.js" async></script>
  <script src="./js/scripts.js" async></script>

  <!-- CSS -->
  <link rel="stylesheet" href="./css/style.css">
  <link rel="stylesheet" href="./css/slider_switch.css">
  <link rel="stylesheet" href="./css/humanav_demo.css">


  <title>Visual Navigation in a Dynamic Environment With Optimal Control as a Supervisor</title>
</head>



<body data-gr-c-s-loaded="true">

<!-- Title -->
<br>
<center>
<span style="font-size:42px">Visual Navigation in Dynamic Environments</span>
</center>



<!-- Authors -->
<br><br>
<table align="center" width="900px">
  <tbody>
    <tr>
      <td align="center" width="100px">
        <center>
          <span style="font-size:20px"><a href="https://samtron1412.github.io/">Son Tran*<sup>1</sup></a></span>
        </center>
      </td>

      <td align="center" width="100px">
        <center>
          <span style="font-size:20px"><a href="">Zuyong Li<sup>1</sup></a></span>
        </center>
      </td>

      <td align="center" width="100px">
        <center>
          <span style="font-size:20px"><a href="">Shawn Shacterman<sup>1</sup></a></span>
        </center>
      </td>

      <td align="center" width="100px">
        <center>
          <span style="font-size:20px"><a href="https://www.linkedin.com/in/rexhanh/">Yuanrong Han</a></span>
        </center>
      </td>
    </tr>
  </tbody>
</table>
<!-- School -->
<br>
<table align="center" width="700px">
  <tbody>
    <tr>
      <td align="center" width="100px">
        <center>
          <span style="font-size:15px"><sup>1</sup> University of California, Berkeley</span>
        </center>
      </td>
    </tr>
  </tbody>
</table>



<!-- Image -->
<br><br>
<table align="center">
  <tbody>
    <tr>
      <td width="40%">
        <center>
          #TODO: Add images to grasp the attention
          <!-- <a href="https://github.com/robot-gang/visual-navigation"><img class="round" style="width:35%" src="./img/front_fig_human.png"></a> -->
          <!-- <a href="https://github.com/robot-gang/visual-navigation"><img class="round" style="width:35%" src="./img/front_figure3.jpg"></a> -->
        </center>
      </td>
    </tr>
  </tbody>
</table>



<!-- Abstract -->
<hr>
<center><h1>Abstract</h1></center>
<br>
<body>
  In this paper we extend recently proposed solutions for tackling autonomous
  navigation in unknown environments, and demonstrate a method for identifying
  dynamic obstacles only using information from an RGB-D camera. We also extend
  current state-of-the-art methods for quickly generating simulated scenarios in
  which a robot must navigate to a goal while avoiding moving obstacles. Finally,
  we propose an approach for combining the identification of dynamic obstacles
  with these random scenarios to train a network to accurately predict waypoints
   on the path from a start position to a goal position in unknown environments.
</body>
<br><br>
<!-- Paper -->
<hr>
<!-- <table align=center width=550px> -->
<center><h1>Paper</h1></center>
<table align="center" width="650">
  <tbody>
    <tr>
      <!-- <td><a href="https://arxiv.org/pdf/xxxx.xxxxx.pdf"><img style="height:280px;border:2px solid #dddddd; border-radius: 5px;" src="./img/paper.jpg"></a></td> -->
      #TODO: Add an image of this paper
      <td>
        <span style="font-size:14pt">
          Tran, Li, Shacterman, Han<br><br>
          Visual Navigation in a Dynamic Environment With Optimal Control as a Supervisor
        </span>
      </td>
    </tr>
  </tbody>
</table>

<br>
<table align="center" width="180px">
  <tbody>
    <tr>
      <td><span style="font-size:14pt"><center>
            <a href="https://arxiv.org/pdf/xxxx.xxxxx.pdf">[pdf]</a>
          </center></span></td>

          <td><span style="font-size:14pt"><center>
                <a href="https://robot-gang.github.io/visual-navigation/resources/bibtex.bib">[Bibtex]</a>
              </center></span></td>
    </tr>
  </tbody>
</table>
<br>



<!-- Architecture and Code -->
<hr>
<center><h1>Architecture &amp; Code</h1></center>
<!-- <p>We build off of the <a href="https://smlbansal.github.io/LB-WayPtNav/">LB-WayPtNav framework</a> for visual navigation which combines optimal control and learning for efficient navigation in novel, indoor environment. In this work we present LB-WayPtNav-DH, which is trained using photorealistic images generated from the HumANav Dataset (below).</p> -->
#TODO: Add some description and an image of the architecture
<center>
  <!-- <a href="https://github.com/robot-gang/visual-navigation"><img class="round" style="height:400" src="./img/lb_wayptnav_framework.png"></a> -->
</center>
<br>
<center>
  <span style="font-size:28px">&nbsp;<a href="https://github.com/robot-gang/visual-navigation">[Code Coming Soon!]</a>
</center>



<!-- Dataset -->
<hr>
<center><h1>Dataset</h1></center>
<center>
  <!-- <a href="https://github.com/robot-gang/visual-navigation"><img class="round" style="width:70%" src="./img/dataset.jpg"></a> -->
  <a href="https://samtron1412.github.io/visual-navigation/"><img class="round" style="width:70%" src="./img/data_gen.png"></a>
</center>
<br>
  <body>
     We decide to use HumANav to generate synthetic human and environment to make
     data generation easier. So HumANav Dataset. It combines SURREAL engine and
     Swiftshader engine using SD3DIS building data. SURREAL generates synthetic
     but realistic images of people, and it also provides RGB data and disparity.
     HumANav uses the Stanford Large Scale 3d Indoor Spaces Dataset, since the mesh
     in the dataset is independent from the rendering engine, HumANav combines the
     SURREAL human mesh with the S3DIS to generate a synthetic human image in an
     indoor environment.
  </body>

<br>
<center>
  <span style="font-size:28px">&nbsp;<a href="https://github.com/robot-gang/visual-navigation">[GitHub]</a></span>
</center>
<!-- Results -->
<hr>
<center><h1>Implementation/Results</h1></center>
<br>
<body>
  <h3> Data Generation</h3>
  <p>For generating data, we use HumANav to generate photorealistic synthetic indoor
  data. We have written a trajectory class to generating a sequence of images
  that human moving in a straight line or turning (circle trajectory).</p>
  <center>
  <a href="https://samtron141ad2.github.io/visual-navigation/"><img class="round" style="width:40%" src="./img/Moving_Human_Line.gif"></a>
  <center>
</body>
<br>
<!-- Conclusion -->
<hr>
<center><h1>Conclusion</h1></center>
<br>
#TODO: Add a conclusion
<br><br>



<!-- Team Bios -->
<hr>
<center><h1>Team Bios</h1></center>
<br>
#TODO: Add team bios
<br><br>



<!-- Acknowledgements -->
<hr>
<table align="center" width="1100px">
  <tbody>
    <tr>
      <td>
        <left>
        <center><h1>Acknowledgements</h1></center>

        #TODO: Add acknowledgements

        <br>
        <br>

        This webpage template was borrowed from <a href="https://richzhang.github.io/colorization/">here</a>.
        </left>
      </td>
    </tr>
  </tbody>
</table>
<br>
<br>
</body>
</html>
