<html>
<head>
  <!-- Metas -->
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/>
  <meta name="author" content="Son Tran"/>
  <meta name="description" content="Final Project EE106B, Spring 2020, UC Berkeley"/>
  <meta name="keywords" content="Robot, Robotics, Vision, Computer Vision, Control, MPC, Learning, Feedback Control"/>

  <!-- JavaScripts -->
  <!-- <script src="./index_files/jsapi"></script> -->
  <!-- <script>google.load("jquery", "1.3.2");</script> -->
  <script src="./js/humanav_demo.js" async></script>
  <script src="./js/scripts.js" async></script>

  <!-- CSS -->
  <link rel="stylesheet" href="./css/style.css">
  <link rel="stylesheet" href="./css/slider_switch.css">
  <link rel="stylesheet" href="./css/humanav_demo.css">


  <title>Visual Navigation in a Dynamic Environment With Optimal Control as a Supervisor</title>
</head>



<body data-gr-c-s-loaded="true">

<!-- Title -->
<br>
<center>
<span style="font-size:42px">Visual Navigation in a Dynamic Environment With <br> Optimal Control as a Supervisor</span>
</center>



<!-- Authors -->
<br><br>
<table align="center" width="900px">
  <tbody>
    <tr>
      <td align="center" width="100px">
        <center>
          <span style="font-size:20px"><a href="https://samtron1412.github.io/">Son Tran*<sup>1</sup></a></span>
        </center>
      </td>

      <td align="center" width="100px">
        <center>
          <span style="font-size:20px"><a href="">Zuyong Li<sup>1</sup></a></span>
        </center>
      </td>

      <td align="center" width="100px">
        <center>
          <span style="font-size:20px"><a href="">Shawn Shacterman<sup>1</sup></a></span>
        </center>
      </td>

      <td align="center" width="100px">
        <center>
          <span style="font-size:20px"><a href="https://www.linkedin.com/in/rexhanh/">Yuanrong Han</a></span>
        </center>
      </td>
    </tr>
  </tbody>
</table>
<!-- School -->
<br>
<table align="center" width="700px">
  <tbody>
    <tr>
      <td align="center" width="100px">
        <center>
          <span style="font-size:15px"><sup>1</sup> University of California, Berkeley</span>
        </center>
      </td>
    </tr>
  </tbody>
</table>



<!-- Image -->
<br><br>
<table align="center">
  <tbody>
    <tr>
      <td width="40%">
        <center>
          #TODO: Add images to grasp the attention
          <!-- <a href="https://github.com/robot-gang/visual-navigation"><img class="round" style="width:35%" src="./img/front_fig_human.png"></a> -->
          <!-- <a href="https://github.com/robot-gang/visual-navigation"><img class="round" style="width:35%" src="./img/front_figure3.jpg"></a> -->
        </center>
      </td>
    </tr>
  </tbody>
</table>



<!-- Abstract -->
<hr>
<center><h1>Abstract</h1></center>
<br>
<body>
  In this paper we extend recently proposed solutions for tackling autonomous
  navigation in unknown environments, and demonstrate a method for identifying
  dynamic obstacles only using information from an RGB-D camera. We also extend
  current state-of-the-art methods for quickly generating simulated scenarios in
  which a robot must navigate to a goal while avoiding moving obstacles. Finally,
  we propose an approach for combining the identification of dynamic obstacles
  with these random scenarios to train a network to accurately predict waypoints
   on the path from a start position to a goal position in unknown environments.
</body>
<br><br>
<!-- Paper -->
<hr>
<!-- <table align=center width=550px> -->
<center><h1>Paper</h1></center>
<table align="center" width="650">
  <tbody>
    <tr>
      <!-- <td><a href="https://arxiv.org/pdf/xxxx.xxxxx.pdf"><img style="height:280px;border:2px solid #dddddd; border-radius: 5px;" src="./img/paper.jpg"></a></td> -->
      #TODO: Add an image of this paper
      <td>
        <span style="font-size:14pt">
          Tran, Li, Shacterman, Han<br><br>
          Visual Navigation in a Dynamic Environment With Optimal Control as a Supervisor
        </span>
      </td>
    </tr>
  </tbody>
</table>

<br>
<table align="center" width="180px">
  <tbody>
    <tr>
      <td><span style="font-size:14pt"><center>
            <a href="https://arxiv.org/pdf/xxxx.xxxxx.pdf">[pdf]</a>
          </center></span></td>

          <td><span style="font-size:14pt"><center>
                <a href="https://robot-gang.github.io/visual-navigation/resources/bibtex.bib">[Bibtex]</a>
              </center></span></td>
    </tr>
  </tbody>
</table>
<br>



<!-- Architecture and Code -->
<hr>
<center><h1>Architecture &amp; Code</h1></center>
<!-- <p>We build off of the <a href="https://smlbansal.github.io/LB-WayPtNav/">LB-WayPtNav framework</a> for visual navigation which combines optimal control and learning for efficient navigation in novel, indoor environment. In this work we present LB-WayPtNav-DH, which is trained using photorealistic images generated from the HumANav Dataset (below).</p> -->
#TODO: Add some description and an image of the architecture
<center>
  <!-- <a href="https://github.com/robot-gang/visual-navigation"><img class="round" style="height:400" src="./img/lb_wayptnav_framework.png"></a> -->
</center>
<br>
<center>
  <span style="font-size:28px">&nbsp;<a href="https://github.com/robot-gang/visual-navigation">[Code Coming Soon!]</a>
</center>



<!-- Dataset -->
<hr>
<center><h1>Dataset</h1></center>
<center>
  <!-- <a href="https://github.com/robot-gang/visual-navigation"><img class="round" style="width:70%" src="./img/dataset.jpg"></a> -->
  <a href="https://samtron1412.github.io/visual-navigation/"><img class="round" style="width:70%" src="./img/data_gen.png"></a>
</center>
<br>
  <body>
     we decide to use HumANav to generate synthetic human and environment to make
     data generation easier. So HumANav Dataset. It combines SURREAL engine and
     Swiftshader engine using SD3DIS building data. SURREAL generates synthetic
     but realistic images of people, and it also provides RGB data and disparity.
     HumANav uses the Stanford Large Scale 3d Indoor Spaces Dataset, since the mesh
     in the dataset is independent from the rendering engine, HumANav combines the
     SURREAL human mesh with the S3DIS to generate a synthetic human image in an
     indoor environment.
  </body>

<br>
<center>
  <span style="font-size:28px">&nbsp;<a href="https://github.com/robot-gang/visual-navigation">[GitHub]</a></span>
</center>
<!-- Results -->
<hr>
<center><h1>Results (Simulation)</h1></center>
#TODO: Add results

<!-- <br> -->
<!-- <table align="center" width="900px"> -->
<!--   <tbody> -->
<!--     <tr> -->
<!--       <td width="600px"> -->
<!--         <center> -->
<!--           <img src="./index_files/sim_success_percent.jpg" width="60%"><br> -->
<!--         </center> -->
<!--       </td> -->
<!--     </tr> -->
<!--     <tr> -->
<!--       <td> -->
<!--         <center> -->
<!--           <span style="font-size:14px"><i> -->
<!--               <span style="font-weight:bold">Metrics</span> We evaluate the success rate of LB-WayPtNav-DH against 7 baselines on a test set of 154 navigational goals in a <i>never-before-seen</i> building, with <i>held-out</i> human identities. Here we discuss comparisons with <a href="https://smlbansal.github.io/LB-WayPtNav/">LB-WayPtNav</a>, the original algorithm trained on static environments (no humans), and Mapping-SH (Static Human), a purely geometric mapping and planning method which projects the robot's current depth image onto the ground plane (treating all obstacles, including humans, as static obstacles) and uses this occupancy grid for path planning. -->

<!--             </i></span> -->
<!--         </center> -->
<!--       </td> -->
<!--     </tr> -->
<!--   </tbody> -->
<!-- </table> -->

<!-- <br> -->
<!-- Overall we find that LBWayPtNav-DH is able to learn plan efficient goal-driven trajectories in novel environments while reasoning about the dynamic nature of humans. LBWayPtNav-DH significantly outperforms the other learning-based baselines in simulation. We also compare to several other baselines (not shown here) including other variants of LB-WayPtNav trained on different datasets, an End-to-End learning method trained to predict optimal control commands directly from images, and Mapping-WC (WC), a geometric mapping and planning method which operates similarly to Mapping-SH, but accounts for the human's future motion by planning a path around their worst case future behavior. For a complete, detailed discussion of the results please see the <a href="https://smlbansal.github.io/LB-WayPtNav-DH/index.html">paper</a>. -->
<!-- <br> -->
<!-- <br> -->
<!-- <table align="center" width="900px"> -->
<!--   <tbody> -->
<!--     <tr> -->
<!--       <td width="600px"> -->
<!--         <center> -->
<!--           <img src="./index_files/lbwayptnav_dh_vs_no_human.jpg" height="40%"><br> -->
<!--         </center> -->
<!--       </td> -->
<!--     </tr> -->
<!--     <tr> -->
<!--       <td width="600px"> -->
<!--         <center> -->
<!--           <span style="font-size:14px"><i>(Left) We visualize a topview of the trajectories taken by LBWayPtNav- -->
<!--               DH (red- solid line) and LB-WayPtNav (light blue- dashed line) on -->
<!--               one representative navigational goal which requires nuanced reasoning about -->
<!--               the directionality of the human. Both agents start at the dark-blue circle and -->
<!--               their goal is to reach the green circle while avoiding static obstacles (dark -->
<!--               grey) and a human in the environment (magenta). Both LB-WayPtNav-DH -->
<!--               and LB-WayPtNav behave similarly at the beginning of their trajectories, but -->
<!--               diverge upon seeing the human (red circle and light blue circle respectively on -->
<!--               the top view plot). The RGB images the agents see of the human are shown -->
<!--               on the right. LB-WayPtNav plans a path to the right of the human (in its -->
<!--               direction of motion), ultimately leading to collision. LB-WayPtNav-DH plans -->
<!--               a trajectory (transparent red) to the left the of the dynamic agent, accounting -->
<!--               for the human’s future motion, and ultimately leading to its success.</i></span> -->
<!--         </center> -->
<!--       </td> -->
<!--     </tr> -->
<!--   </tbody> -->
<!-- </table> -->


<!-- <br> -->
<!-- <table align="center" width="900px"> -->
<!--   <tbody> -->
<!--     <tr> -->
<!--       <td width="600px"> -->
<!--         <center> -->
<!--           <img src="./index_files/simulation_legs_apart.jpg" height="40%"><br> -->
<!--         </center> -->
<!--       </td> -->
<!--     </tr> -->
<!--     <tr><td width="600px"> -->
<!--         <center> -->
<!--           <span style="font-size:14px"><i> (Left) We visualize a topview of the trajectories taken by LBWayPtNav- -->
<!--               DH from the same state with a static human (light blue- dashed -->
<!--               line) and with a dynamic human (red- solid line). The corresponding RGB -->
<!--               images seen by the robot are shown on the right. LB-WayPtNav-DH is able -->
<!--               to incorporate visual cues, i.e. spread of humans legs and direction of human -->
<!--               toes, into path planning, planning a path which avoids the human’s current -->
<!--               and future states.</i> -->
<!--           </span></center> -->
<!--       </td> -->
<!--     </tr> -->
<!--   </tbody> -->
<!-- </table> -->



<!-- <hr> -->
<!-- <center><h1>Hardware Experiments</h1></center> -->

<!-- <br> -->
<!-- <table align="center" width="900px"> -->
<!--   <tbody> -->
<!--     <tr> -->
<!--       <td align="center"> -->
<!--         <img src="./index_files/front_fig_human(1).png" width="350px"><br> -->
<!--       </td> -->
<!--       <td> -->
<!--         We deploy our simulation trained algorithm on a Turtlebot 2 to test on real-world navigational scenarios. Each experiment is visualized from three different viewpoints, however the robot only sees the "First Person View" (also labeled Robot View). The other two viewpoints are provided for context only. We do not train or finetune our algorithm in any way on real data. All experiments are shown in realtime. -->
<!--         <br><br> -->
<!--         We compare the performance of LB-WayPtNav-DH, LBWayPtNav, -->
<!--         and Mapping-SH on our hardware platform across -->
<!--         two experimental settings for five trials each (10 runs total). Quantitative -->
<!--         results are presented below. We do not compare to End-To- -->
<!--         End or Mapping-WC on our hardware setup as the simulation -->
<!--         performance of End-To-End is already very low and Mapping- -->
<!--         WC requires access to the ground truth state information of the -->
<!--         human, which we noticed was not reliable using our narrow -->
<!--         field-of-view monocular RGB camera. -->
<!--       </td> -->
<!--     </tr> -->
<!--   </tbody> -->
<!-- </table> -->

<!-- <br> -->
<!-- <br> -->
<!-- <table align="center" width="900px"> -->
<!--   <tbody> -->
<!--     <tr> -->
<!--       <td width="600px"> -->
<!--         <center> -->
<!--           <img src="./index_files/experiment_results.jpg" width="800px"><br> -->
<!--         </center> -->
<!--       </td> -->
<!--     </tr> -->
<!--     <tr><td width="600px"> -->
<!--         <center> -->
<!--           <span style="font-size:14px"><i> <span style="font-weight:bold">Metrics</span> In real-world experiments LB-WayPtNav-DH continues to perform well, however performance of both LB-WayPtNav and Mapping-SH degrades significantly as they do not take into account the <span style="font-weight:bold">dynamic</span> nature of the human. When Mapping-SH does succeed, it reaches the goal significantly faster than LBWayPtNav-DH and LB-WayPtNav, as it is able to exploit the precise geometry of the scene and barely avoid collision with the human. It does so, by executing a "last-minute" aggressive stopping manouvre to avoid collision, explaining the high jerk of Mapping-SH.</i> -->
<!--           </span> -->
<!--         </center> -->
<!--       </td> -->
<!--     </tr> -->
<!--   </tbody> -->
<!-- </table> -->


<!-- <br> -->
<!-- <center><h2>Videos</h2></center> -->
<!-- <table align="center"> -->
<!--   <tbody> -->
<!--     <tr> -->
<!--       <td> -->
<!--         <center> -->
<!--           <iframe width="600" height="338" src="./index_files/wYisoZBu1Y0.html" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br> -->
<!--         </center> -->
<!--       </td> -->
<!--     </tr> -->
<!--     <tr> -->
<!--       <td width="800px"> -->
<!--         <center> -->
<!--           <span style="font-size:14px"><i> <span style="font-weight:bold">Experiments 1: </span>The robot is tasked with moving 5 meters forward, while avoiding the human. Roughly halfway through its trajectory, the human decides to change direction, forcing the robot to react. LB-WayPtNav-DH reasons about the human's short-term future trajectory predicting a waypoint which ultimately avoids collision by moving to the right. LB-WayPtNav and Mapping-SH treat the human as a static obstacle, predict a waypoint to the left of the human (in its direction of motion) and ultimately collide.</i></span> -->
<!--           <br><br><br> -->
<!--         </center> -->
<!--       </td> -->
<!--     </tr> -->

<!--     <tr> -->
<!--       <td> -->
<!--         <center> -->
<!--           <iframe width="600" height="338" src="./index_files/mZ65ZgjI3yQ.html" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br> -->
<!--         </center> -->
<!--       </td> -->
<!--     </tr> -->
<!--   </tbody> -->
<!-- </table> -->

<!-- <table align="center"> -->
<!--   <tbody> -->
<!--     <tr> -->
<!--       <td width="800px"> -->
<!--         <center> -->
<!--           <span style="font-size:14px"><i> <span style="font-weight:bold">Experiment 2: </span>The robot is tasked with navigating to a goal down the hallway and around the corner, however there is also a human walking around the corner towards the robot. LB-WayPtNav-DH takes a more cautious trajectory around the corner, and thus is able to react and avoid the human. Mapping-SH and LB-WayPtNav both try to take aggressive trajectories around the corner guiding them almost head on into the human. When the do finally recognize the human they treat it as a static obstacle and ultimately collide. </i></span> -->
<!--           <br><br><br> -->
<!--         </center> -->
<!--       </td> -->
<!--     </tr> -->
<!--   </tbody> -->
<!-- </table> -->



<!-- Conclusion -->
<hr>
<center><h1>Conclusion</h1></center>
<br>
#TODO: Add a conclusion
<br><br>



<!-- Team Bios -->
<hr>
<center><h1>Team Bios</h1></center>
<br>
#TODO: Add team bios
<br><br>



<!-- Acknowledgements -->
<hr>
<table align="center" width="1100px">
  <tbody>
    <tr>
      <td>
        <left>
        <center><h1>Acknowledgements</h1></center>

        #TODO: Add acknowledgements

        <br>
        <br>

        This webpage template was borrowed from <a href="https://richzhang.github.io/colorization/">here</a>.
        </left>
      </td>
    </tr>
  </tbody>
</table>
<br>
<br>
</body>
</html>
