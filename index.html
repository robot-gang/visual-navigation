<html>
<head>
  <!-- Metas -->
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/>
  <meta name="author" content="Son Tran"/>
  <meta name="description" content="Final Project EE106B, Spring 2020, UC Berkeley"/>
  <meta name="keywords" content="Robot, Robotics, Vision, Computer Vision, Control, MPC, Learning, Feedback Control"/>

  <!-- JavaScripts -->
  <!-- <script src="./index_files/jsapi"></script> -->
  <!-- <script>google.load("jquery", "1.3.2");</script> -->
  <script src="./js/humanav_demo.js" async></script>
  <script src="./js/scripts.js" async></script>

  <!-- CSS -->
  <link rel="stylesheet" href="./css/style.css">
  <link rel="stylesheet" href="./css/slider_switch.css">
  <link rel="stylesheet" href="./css/humanav_demo.css">


  <title>Visual Navigation in a Dynamic Environment With Optimal Control as a Supervisor</title>
</head>



<body data-gr-c-s-loaded="true">

<!-- Title -->
<br>
<center>
<span style="font-size:42px">Visual Navigation in Dynamic Environments</span>
</center>



<!-- Authors -->
<br><br>
<table align="center" width="900px">
  <tbody>
    <tr>
      <td align="center" width="100px">
        <center>
          <span style="font-size:20px"><a href="https://samtron1412.github.io/">Son Tran*<sup>1</sup></a></span>
        </center>
      </td>

      <td align="center" width="100px">
        <center>
          <span style="font-size:20px"><a href="">Zuyong Li<sup>1</sup></a></span>
        </center>
      </td>

      <td align="center" width="100px">
        <center>
          <span style="font-size:20px"><a href="">Shawn Shacterman<sup>1</sup></a></span>
        </center>
      </td>

      <td align="center" width="100px">
        <center>
          <span style="font-size:20px"><a href="https://www.linkedin.com/in/rexhanh/">Yuanrong Han</a></span>
        </center>
      </td>
    </tr>
  </tbody>
</table>
<!-- School -->
<br>
<table align="center" width="700px">
  <tbody>
    <tr>
      <td align="center" width="100px">
        <center>
          <span style="font-size:15px"><sup>1</sup> University of California, Berkeley</span>
        </center>
      </td>
    </tr>
  </tbody>
</table>



<!-- Image -->
<br><br>
<table align="center">
  <tbody>
    <tr>
      <td width="40%">
        <center>
          #TODO: Add images to grasp the attention
          <!-- <a href="https://github.com/robot-gang/visual-navigation"><img class="round" style="width:35%" src="./img/front_fig_human.png"></a> -->
          <!-- <a href="https://github.com/robot-gang/visual-navigation"><img class="round" style="width:35%" src="./img/front_figure3.jpg"></a> -->
        </center>
      </td>
    </tr>
  </tbody>
</table>



<!-- Abstract -->
<hr>
<center><h1>Overview</h1></center>
<br>
<body>
  We extend recently proposed solutions for tackling autonomous
  navigation in unknown environments, and demonstrate a method for identifying
  dynamic obstacles only using information from an RGB-D camera.
  we successfully extended the HumANav dataset to allow for a wider
  range of scenarios that can be used to train our perception module. We designed
  an algorithm that successfully and accurately detects the moving objects by
  combining changes in intensity and depth.

  The result of data generation and moving object detection look appealing and
  encouraging. The end goal is to design and train a neural network for waypoint
  detection and integrating all modules of our design and applying to hardwares.
  This would solve the autonomous navigation in dynamic unknown environments with
  low cost sensors. Especially, it would work for indoor robots which operate
  at a low speed and the environments are most likely to be dynamic. Warehouse robots
  would benefit the most.
</body>
<br><br>
<!-- Architecture and Code -->
<hr>
<center><h1>Architecture &amp; Code</h1></center>
<!-- <p>We build off of the <a href="https://smlbansal.github.io/LB-WayPtNav/">LB-WayPtNav framework</a> for visual navigation which combines optimal control and learning for efficient navigation in novel, indoor environment. In this work we present LB-WayPtNav-DH, which is trained using photorealistic images generated from the HumANav Dataset (below).</p> -->
#TODO: Add some description and an image of the architecture
<center>
  <!-- <a href="https://github.com/robot-gang/visual-navigation"><img class="round" style="height:400" src="./img/lb_wayptnav_framework.png"></a> -->
</center>
<br>
<center>
  <span style="font-size:28px">&nbsp;<a href="https://github.com/robot-gang/visual-navigation">[Code Coming Soon!]</a>
</center>



<!-- Dataset -->
<hr>
<center><h1>Dataset</h1></center>
<center>
  <!-- <a href="https://github.com/robot-gang/visual-navigation"><img class="round" style="width:70%" src="./img/dataset.jpg"></a> -->
  <a href="https://samtron1412.github.io/visual-navigation/"><img class="round" style="width:70%" src="./img/data_gen.png"></a>
</center>
<br>
  <body>
     We decide to use HumANav to generate synthetic human and environment to make
     data generation easier. So HumANav Dataset. It combines SURREAL engine and
     Swiftshader engine using SD3DIS building data. SURREAL generates synthetic
     but realistic images of people, and it also provides RGB data and disparity.
     HumANav uses the Stanford Large Scale 3d Indoor Spaces Dataset, since the mesh
     in the dataset is independent from the rendering engine, HumANav combines the
     SURREAL human mesh with the S3DIS to generate a synthetic human image in an
     indoor environment.
  </body>

<br>
<center>
  <span style="font-size:28px">&nbsp;<a href="https://github.com/robot-gang/visual-navigation">[GitHub]</a></span>
</center>
<!-- Results -->
<hr>
<center><h1>Implementation/Results</h1></center>
<body>
  <h3> Data Generation</h3>
  <p>
    For generating data, we use HumANav to generate photorealistic synthetic indoor
    moving human images. HumANav combines SURREAL engine and Swiftshader engine using
    SD3DISbuilding data . SURREAL generates synthetic but realisticimages of
    people, and it also provides RGB data and disparity. HumANav uses the Stanford
    Large Scale 3D Indoor SpacesDataset, since the mesh in the dataset is
    independent fromthe rendering engine, HumANav combines the SURREAL human mesh
    with the S3DIS to generate a synthetic humanimage in an indoor environment.
    HumANav Dataset contains6000 different human meshes with different poses,
    genders,body shapes, lighting conditions and velocities.HumANavWe have written
    a trajectory class to generating a sequence of images that human moving in a
    straight line or turning (circle trajectory) at a constant velocity. We also
    write code to random generate human in the environment, and also we add support
    for multiple human in a scene where HumANav does not support. In the figure
    above, left is a RGB image, in the middle is a disparity map and on the right
    is the top down view, and all three images are generated automatically.
  <br>
  <center>
    <a href="https://samtron141ad2.github.io/visual-navigation/"><img class="round" style="width:35%; position:left" src="./img/moving_human_line.gif"></a>
    <a href="https://samtron141ad2.github.io/visual-navigation/"><img class="round" style="width:35%; position:right" src="./img/moving_human_circle.gif"></a>
  </center>
  <br>
  We also write code to random generate human in the environment, and also we
  add support for multiple human in a scene where HumANav does not support. We
  make the area around each human darker to indicate more strictive feasible area
  for the robot.
  <center>
    <a href="https://samtron141ad2.github.io/visual-navigation/"><img class="round" style="width:90%" src="./img/multiple_humans.png"></a>
  </center>
  </p>
  <h3> Motion Detection</h3>
  <p>
  For motion detection, there are four main parts shaded in pink in the diagram:
  feature matching, detecting changes in intensity, detecting changes in depth, and
  combination of changes in intensity and depth. To match the features, we use FAST
  detector, BRIEF descriptor, and BFMatcher with 2 nearest neighbors and apply ratio
  test to determine the good matches. To detect changes in intensity, we run ransac
  on the matched features to estimate homography matrix H and use H to warp image A
  into image B's projection plane to obtain A'. Apply Gaussian filter and normalize
  A' and B, then do the subtraction to get the change. To detect changes in intensity,
  we run first convert the feature points to its local 3D coordinates using depth
  arrays A and B (the have the same name because the math and scheme are the same).
  Then we can estimate the rigid body transformation (R, t) of the camera by runing ransac
  on the local 3D correspondence points. Depth A is reprojected into B's local frame
  A'. Apply Gaussian filter and normalize A' and B, then do the subtraction. The
  absolute value of the difference is the change in depth. To combine the result of
  changes in intensity and depth, simply do a linear combination, or average of change
  in intensity and depth. Applying a threshold produces the output.
  </p>
  <br>
  <p>
    <center>
      <a href="https://samtron141ad2.github.io/visual-navigation/"><img class="round" style="width:90%" src="./img/flowchart.png"></a>
    </center>
  </p>
  <p>
    The input gray-scale images and depth arrays are obtained from an Intel
    RealSense depth camera D435i, which is mounted on a robot car (designed by
    the Robot Open Autonomous Racing team) about $10cm$ above the ground.
  </p>
  <br>
  <p>
    <center>
      <a href="https://samtron141ad2.github.io/visual-navigation/"><img class="round" style="width:90%" src="./img/motion_result_0.png"></a>
    </center>
  </p>
  <br>
  The camera pose has changed by inspecting
  the tiles on the ground. The bottom intersection of tile lines in B is
  closer to the image border. The person is walking by inspecting the change
  of his pose. In the left image, his left hand is shown; whereas his left
  hand is covered in the right image.
  <p>
    <center>
      <a href="https://samtron141ad2.github.io/visual-navigation/"><img class="round" style="width:70%" src="./img/motion_result_1.png"></a>
    </center>
    Motion detection result: (a), the changes in intensity. It also
    captures the intensity change of the floor which is caused by the change of
    camera pose (b), the changes in depth Delta Lambda. The depth is
    sensitive to the setting of environment. The change of depth captures
    the border of the walking person and a lot of random stuff from the
    background. (c), combination of change in intensity and depth produces a
    better and clean detection, where intensity and depth are weighted equally.
    <br>
    <center>
      <iframe src="https://drive.google.com/file/d/1a-NHYCNLTzaQiCwm-v8LtMOXBqZc2V_Y/preview" width="640" height="480"></iframe>
    </center>
  </p>
</body>
<!-- Conclusion -->
<hr>
<center><h1>Conclusion</h1></center>
<br>
In this project, we successfully extended the HumANav dataset to allow for a wider
range of scenarios that can be used to train our perception module. Rather than having
our obstacles take an optimal trajectory, our dataset allows us to set arbitrary
trajectories along the free space of the occupancy map. Our approach of combining changes
in intensity and depth successfully detects the moving objects. Changes in intensity
captures both the motion of object and camera. Changes in depth captures the border of
the moving objects. Linear combination of intensity and depth produces a cleaner
detection better than either of them.

The main chanllege of detecting moving objects is how to compare two RGB/grayscale
images or depth arrays. The first task is to make sure that those two input intenseity
are viewed in the same projection plane, and the two input depth array are seen from the
same camera pose. It was solved by using perspective warping and reprojection. It took a long
time to find out that Gaussian filter and normalization before comparison are crucial. 

More work needs to be done in determining a good method for sampling different
obstacle trajectories that the vehicle is likely to encounter in the real world.
his will be essential in generating a good dataset that our perception module can learn
and generalize from. We also need to experiment with different neural networks in the
perception module. At the moment, we are unsure whether a normal CNN, a CNN-LSTM,
or something different like a temporal CNN will generalize well in this task
(and ideally perform well in real-time).
<br><br>
<!-- Team Bios -->
<hr>
<center><h1>Team Bios</h1></center>
<br>
Zuyong Li, a senior EECS student who is graduating in Spring 2020. He is interested
in robotics, computer vision and machine learning. He worked mainly for moving object
detection algorithm design, and implemented the algorithm together with Son Tran. He
also collected the data for moving object detection using a ROAR car.
<br><br>
Shawn Shacterman, a senior EECS student who is graduating in Fall 2020. He is interested
robotics, machine learning, and drinking alone. He worked on extending the rendering engine
for the HumANav dataset to allow for multiple humans in the environment. He also worked a
little on generating basic trajectories for human obstacles.
<br><br>
Yuanrong Han, usually he goes by Rex. He is a senior EECS student who is graduating in Fall 2020.
He is interested in robotics, machine learning, and cooking. He worked with Shawn on extending
the rendering engine from HumANav dataset, and he also helped to write a class to generate a
sequence of images in different trajectories.
<br><br>
<!-- Acknowledgements -->
<hr>
<table align="center" width="1100px">
  <tbody>
    <tr>
      <td>
        <left>
        <center><h1>Acknowledgements</h1></center>

        We thank Professor Shankar Sastry and the course staff for assistance with the project
        idea for the problem. We thank the Robot Open Autonomous Racing team for providing us a
        robot car and calibrating the RealSense camera. We also thank Nghia Ho for sharing the code
        how to compute the rigid body transformation for given 3D correspondence points.

        <br>
        <br>

        This webpage template was borrowed from <a href="https://richzhang.github.io/colorization/">here</a>.
        </left>
      </td>
    </tr>
  </tbody>
</table>
<br>
<br>
</body>
</html>
