<html>
<head>
  <!-- Metas -->
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/>
  <meta name="author" content="Son Tran"/>
  <meta name="description" content="Final Project EE106B, Spring 2020, UC Berkeley"/>
  <meta name="keywords" content="Robot, Robotics, Vision, Computer Vision, Control, MPC, Learning, Feedback Control"/>

  <!-- JavaScripts -->
  <!-- <script src="./index_files/jsapi"></script> -->
  <!-- <script>google.load("jquery", "1.3.2");</script> -->
  <script src="./js/humanav_demo.js" async></script>
  <script src="./js/scripts.js" async></script>

  <!-- CSS -->
  <link rel="stylesheet" href="./css/style.css">
  <link rel="stylesheet" href="./css/slider_switch.css">
  <link rel="stylesheet" href="./css/humanav_demo.css">


  <title>Visual Navigation in a Dynamic Environment With Optimal Control as a Supervisor</title>
</head>



<body data-gr-c-s-loaded="true">

<!-- Title -->
<br><br>
<center>
<span style="font-size:42px">Visual Navigation in Dynamic Environments</span>
</center>
<br><br>



<!-- Authors -->
<table align="center" width="900px">
  <tbody>
    <tr>
      <td align="center" width="100px">
        <center>
          <span style="font-size:20px"><a href="https://samtron1412.github.io/">Son Tran*<sup>1</sup></a></span>
        </center>
      </td>

      <td align="center" width="100px">
        <center>
          <span style="font-size:20px"><a href="">Zuyong Li<sup>1</sup></a></span>
        </center>
      </td>

      <td align="center" width="100px">
        <center>
          <span style="font-size:20px"><a href="">Shawn Shacterman<sup>1</sup></a></span>
        </center>
      </td>

      <td align="center" width="100px">
        <center>
          <span style="font-size:20px"><a href="https://www.linkedin.com/in/rexhanh/">Yuanrong Han<sup>1</sup></a></span>
        </center>
      </td>
    </tr>
  </tbody>
</table>
<br><br>
<!-- School -->
<center>
  <span style="font-size:15px"><sup>1</sup> University of California, Berkeley</span>
</center>
<br><br>
<!-- Image -->
<center>
  <a href="https://github.com/robot-gang/visual-navigation"><img class="round" style="width:65%" src="./img/pipeline.png"></a>
</center>
<br><br>
<!-- Abstract -->
<hr>
<center><h1>Overview</h1></center>
<p>
We extend recently proposed solutions for tackling autonomous navigation
in unknown environments, and demonstrate a method for identifying
dynamic obstacles only using information from an RGB-D camera.  we
successfully extended the HumANav dataset to allow for a wider range of
scenarios that can be used to train our perception module. We designed
an algorithm that successfully and accurately detects the moving objects
by combining changes in intensity and depth. The result of data
generation and moving object detection look appealing and
encouraging. The end goal is to design and train a neural network for
waypoint detection and integrating all modules of our design and
applying to hardwares.  This would solve the autonomous navigation in
dynamic unknown environments with low cost sensors. Especially, it would
work for indoor robots which operate at a low speed and the environments
are most likely to be dynamic. Warehouse robots would benefit the most.
</p>
<center>
  <span style="font-size:28px">&nbsp;<a href="./resources/finalproj_paper.pdf" target="_blank">[Our Paper!]</a>
</center>
<br><br>
<!-- Results -->
<hr>
<center><h1>Implementation/Results</h1></center>
<h2> Data Generation</h2>
<p>
For generating data, we use HumANav to generate photorealistic synthetic indoor
moving human images. HumANav combines SURREAL engine and Swiftshader engine using
SD3DISbuilding data . SURREAL generates synthetic but realisticimages of
people, and it also provides RGB data and disparity. HumANav uses the Stanford
Large Scale 3D Indoor SpacesDataset, since the mesh in the dataset is
independent fromthe rendering engine, HumANav combines the SURREAL human mesh
with the S3DIS to generate a synthetic humanimage in an indoor environment.
HumANav Dataset contains6000 different human meshes with different poses,
genders,body shapes, lighting conditions and velocities.HumANavWe have written
a trajectory class to generating a sequence of images that human moving in a
straight line or turning (circle trajectory) at a constant velocity. We also
write code to random generate human in the environment, and also we add support
for multiple human in a scene where HumANav does not support. In the figure
above, left is a RGB image, in the middle is a disparity map and on the right
is the top down view, and all three images are generated automatically.
</p>
<br>
<center>
  <img class="round" style="width:35%; position:left" src="./img/moving_human_line.gif">
  <img class="round" style="width:35%; position:right" src="./img/moving_human_circle.gif">
</center>
<br>
<p>
We also write code to random generate human in the environment, and also we
add support for multiple human in a scene where HumANav does not support. We
make the area around each human darker to indicate more strictive feasible area
for the robot.
</p>
<br>
<center>
  <img class="round" style="width:90%" src="./img/multiple_humans.png">
</center>
<br>
<br>
<center>
  <span style="font-size:28px">&nbsp;<a href="https://github.com/robot-gang/visual-navigation">[GitHub]</a></span>
</center>
<br>

<h2> Motion Detection</h2>
<p>
For motion detection, there are four main parts shaded in pink in the diagram:
feature matching, detecting changes in intensity, detecting changes in depth, and
combination of changes in intensity and depth. To match the features, we use FAST
detector, BRIEF descriptor, and BFMatcher with 2 nearest neighbors and apply ratio
test to determine the good matches. To detect changes in intensity, we run ransac
on the matched features to estimate homography matrix H and use H to warp image A
into image B's projection plane to obtain A'. Apply Gaussian filter and normalize
A' and B, then do the subtraction to get the change. To detect changes in intensity,
we run first convert the feature points to its local 3D coordinates using depth
arrays A and B (the have the same name because the math and scheme are the same).
Then we can estimate the rigid body transformation (R, t) of the camera by runing ransac
on the local 3D correspondence points. Depth A is reprojected into B's local frame
A'. Apply Gaussian filter and normalize A' and B, then do the subtraction. The
absolute value of the difference is the change in depth. To combine the result of
changes in intensity and depth, simply do a linear combination, or average of change
in intensity and depth. Applying a threshold produces the output.
</p>
<br>
<center>
  <img class="round" style="width:90%" src="./img/flowchart.png">
</center>
<br>
<p>
The input gray-scale images and depth arrays are obtained from an Intel
RealSense depth camera D435i, which is mounted on a robot car (designed by
the Robot Open Autonomous Racing team) about 10cm above the ground.
</p>
<p>
The camera pose has changed by inspecting
the tiles on the ground. The bottom intersection of tile lines in B is
closer to the image border. The person is walking by inspecting the change
of his pose. In the left image, his left hand is shown; whereas his left
hand is covered in the right image.
</p>
<br>
<center>
  <img class="round" style="width:90%" src="./img/motion_result_0.png">
</center>
<br>
<p>
Motion detection result: (a), the changes in intensity. It also
captures the intensity change of the floor which is caused by the change of
camera pose (b), the changes in depth Delta Lambda. The depth is
sensitive to the setting of environment. The change of depth captures
the border of the walking person and a lot of random stuff from the
background. (c), combination of change in intensity and depth produces a
better and clean detection, where intensity and depth are weighted equally.
</p>
<br>
<center>
  <img class="round" style="width:70%" src="./img/motion_result_1.png">
</center>
<br>
<p>
The video consists of four parts. The top left is the input grayscale images,
the top right the result of combination of changes in intensity and depth, the
bottom left is the changes in intenstiy, and the bottom right is the changes
in depth.
</p>
<br>
<center>
  <iframe src="https://drive.google.com/file/d/1a-NHYCNLTzaQiCwm-v8LtMOXBqZc2V_Y/preview" width="640" height="480"></iframe>
</center>
<br>
<br>
<center>
  <span style="font-size:28px">&nbsp;<a href="https://github.com/robot-gang/visual-navigation">[GitHub]</a></span>
</center>
<br>
<!-- Conclusion -->
<hr>
<center><h1>Conclusion</h1></center>
<p>
In this project, we successfully extended the HumANav dataset to allow
for a wider range of scenarios that can be used to train our perception
module. Rather than having our obstacles take an optimal trajectory, our
dataset allows us to set arbitrary trajectories along the free space of
the occupancy map. Our approach of combining changes in intensity and
depth successfully detects the moving objects. Changes in intensity
captures both the motion of object and camera. Changes in depth captures
the border of the moving objects. Linear combination of intensity and
depth produces a cleaner detection better than either of them.
</p>
<p>
The main chanllege of detecting moving objects is how to compare two
RGB/grayscale images or depth arrays. The first task is to make sure
that those two input intenseity are viewed in the same projection plane,
and the two input depth array are seen from the same camera pose. It was
solved by using perspective warping and reprojection. It took a long
time to find out that Gaussian filter and normalization before
comparison are crucial.
</p>
<p>
More work needs to be done in determining a good method for sampling
different obstacle trajectories that the vehicle is likely to encounter
in the real world.  his will be essential in generating a good dataset
that our perception module can learn and generalize from. We also need
to experiment with different neural networks in the perception
module. At the moment, we are unsure whether a normal CNN, a CNN-LSTM,
or something different like a temporal CNN will generalize well in this
task (and ideally perform well in real-time).
</p>
<br><br>
<!-- Team Bios -->
<hr>
<center><h1>Team Bios</h1></center>
<p>
Son Tran, a senior EECS student who is graduating in Fall 2020. He is
interested in robotics, computer vision and machine learning. He worked
mainly for moving object detection algorithm design, and implemented the
algorithm together with Zuyong Li. He also took care all things related
to setting up the environments for coding and making this website.
</p>
<p>
Zuyong Li, a senior EECS student who is graduating in Spring 2020. He is interested
in robotics, computer vision and machine learning. He worked mainly for moving object
detection algorithm design, and implemented the algorithm together with Son Tran. He
also collected the data for moving object detection using a ROAR car.
</p>
<p>
Shawn Shacterman, a senior EECS student who is graduating in Fall 2020. He is interested
robotics, machine learning, and drinking alone. He worked on extending the rendering engine
for the HumANav dataset to allow for multiple humans in the environment. He also worked a
little on generating basic trajectories for human obstacles.
</p>
<p>
Yuanrong Han, usually he goes by Rex. He is a senior EECS student who is
graduating in Fall 2020.  He is interested in robotics, machine
learning, and cooking. He worked with Shawn on extending the rendering
engine from HumANav dataset, and he also helped to write a class to
generate a sequence of images in different trajectories.
</p>
<br><br>
<!-- Acknowledgements -->
<hr>
<center><h1>Acknowledgements</h1></center>
<p>
We thank Professor Shankar Sastry and the course staff for assistance
with the project idea for the problem. We thank the Robot Open
Autonomous Racing team for providing us a robot car and calibrating the
RealSense camera. We also thank Nghia Ho for sharing the code how to
compute the rigid body transformation for given 3D correspondence
points.
</p>
<p>
This webpage template was borrowed from <a
  href="https://richzhang.github.io/colorization/">here</a>.
</p>
<br><br>
</body>
</html>
