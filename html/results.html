<!-- Results -->
<hr>
<center><h1>Results (Simulation)</h1></center>
#TODO: Add results

<!-- <br> -->
<!-- <table align="center" width="900px"> -->
<!--   <tbody> -->
<!--     <tr> -->
<!--       <td width="600px"> -->
<!--         <center> -->
<!--           <img src="./index_files/sim_success_percent.jpg" width="60%"><br> -->
<!--         </center> -->
<!--       </td> -->
<!--     </tr> -->
<!--     <tr> -->
<!--       <td> -->
<!--         <center> -->
<!--           <span style="font-size:14px"><i> -->
<!--               <span style="font-weight:bold">Metrics</span> We evaluate the success rate of LB-WayPtNav-DH against 7 baselines on a test set of 154 navigational goals in a <i>never-before-seen</i> building, with <i>held-out</i> human identities. Here we discuss comparisons with <a href="https://smlbansal.github.io/LB-WayPtNav/">LB-WayPtNav</a>, the original algorithm trained on static environments (no humans), and Mapping-SH (Static Human), a purely geometric mapping and planning method which projects the robot's current depth image onto the ground plane (treating all obstacles, including humans, as static obstacles) and uses this occupancy grid for path planning. -->

<!--             </i></span> -->
<!--         </center> -->
<!--       </td> -->
<!--     </tr> -->
<!--   </tbody> -->
<!-- </table> -->

<!-- <br> -->
<!-- Overall we find that LBWayPtNav-DH is able to learn plan efficient goal-driven trajectories in novel environments while reasoning about the dynamic nature of humans. LBWayPtNav-DH significantly outperforms the other learning-based baselines in simulation. We also compare to several other baselines (not shown here) including other variants of LB-WayPtNav trained on different datasets, an End-to-End learning method trained to predict optimal control commands directly from images, and Mapping-WC (WC), a geometric mapping and planning method which operates similarly to Mapping-SH, but accounts for the human's future motion by planning a path around their worst case future behavior. For a complete, detailed discussion of the results please see the <a href="https://smlbansal.github.io/LB-WayPtNav-DH/index.html">paper</a>. -->
<!-- <br> -->
<!-- <br> -->
<!-- <table align="center" width="900px"> -->
<!--   <tbody> -->
<!--     <tr> -->
<!--       <td width="600px"> -->
<!--         <center> -->
<!--           <img src="./index_files/lbwayptnav_dh_vs_no_human.jpg" height="40%"><br> -->
<!--         </center> -->
<!--       </td> -->
<!--     </tr> -->
<!--     <tr> -->
<!--       <td width="600px"> -->
<!--         <center> -->
<!--           <span style="font-size:14px"><i>(Left) We visualize a topview of the trajectories taken by LBWayPtNav- -->
<!--               DH (red- solid line) and LB-WayPtNav (light blue- dashed line) on -->
<!--               one representative navigational goal which requires nuanced reasoning about -->
<!--               the directionality of the human. Both agents start at the dark-blue circle and -->
<!--               their goal is to reach the green circle while avoiding static obstacles (dark -->
<!--               grey) and a human in the environment (magenta). Both LB-WayPtNav-DH -->
<!--               and LB-WayPtNav behave similarly at the beginning of their trajectories, but -->
<!--               diverge upon seeing the human (red circle and light blue circle respectively on -->
<!--               the top view plot). The RGB images the agents see of the human are shown -->
<!--               on the right. LB-WayPtNav plans a path to the right of the human (in its -->
<!--               direction of motion), ultimately leading to collision. LB-WayPtNav-DH plans -->
<!--               a trajectory (transparent red) to the left the of the dynamic agent, accounting -->
<!--               for the human’s future motion, and ultimately leading to its success.</i></span> -->
<!--         </center> -->
<!--       </td> -->
<!--     </tr> -->
<!--   </tbody> -->
<!-- </table> -->


<!-- <br> -->
<!-- <table align="center" width="900px"> -->
<!--   <tbody> -->
<!--     <tr> -->
<!--       <td width="600px"> -->
<!--         <center> -->
<!--           <img src="./index_files/simulation_legs_apart.jpg" height="40%"><br> -->
<!--         </center> -->
<!--       </td> -->
<!--     </tr> -->
<!--     <tr><td width="600px"> -->
<!--         <center> -->
<!--           <span style="font-size:14px"><i> (Left) We visualize a topview of the trajectories taken by LBWayPtNav- -->
<!--               DH from the same state with a static human (light blue- dashed -->
<!--               line) and with a dynamic human (red- solid line). The corresponding RGB -->
<!--               images seen by the robot are shown on the right. LB-WayPtNav-DH is able -->
<!--               to incorporate visual cues, i.e. spread of humans legs and direction of human -->
<!--               toes, into path planning, planning a path which avoids the human’s current -->
<!--               and future states.</i> -->
<!--           </span></center> -->
<!--       </td> -->
<!--     </tr> -->
<!--   </tbody> -->
<!-- </table> -->



<!-- <hr> -->
<!-- <center><h1>Hardware Experiments</h1></center> -->

<!-- <br> -->
<!-- <table align="center" width="900px"> -->
<!--   <tbody> -->
<!--     <tr> -->
<!--       <td align="center"> -->
<!--         <img src="./index_files/front_fig_human(1).png" width="350px"><br> -->
<!--       </td> -->
<!--       <td> -->
<!--         We deploy our simulation trained algorithm on a Turtlebot 2 to test on real-world navigational scenarios. Each experiment is visualized from three different viewpoints, however the robot only sees the "First Person View" (also labeled Robot View). The other two viewpoints are provided for context only. We do not train or finetune our algorithm in any way on real data. All experiments are shown in realtime. -->
<!--         <br><br> -->
<!--         We compare the performance of LB-WayPtNav-DH, LBWayPtNav, -->
<!--         and Mapping-SH on our hardware platform across -->
<!--         two experimental settings for five trials each (10 runs total). Quantitative -->
<!--         results are presented below. We do not compare to End-To- -->
<!--         End or Mapping-WC on our hardware setup as the simulation -->
<!--         performance of End-To-End is already very low and Mapping- -->
<!--         WC requires access to the ground truth state information of the -->
<!--         human, which we noticed was not reliable using our narrow -->
<!--         field-of-view monocular RGB camera. -->
<!--       </td> -->
<!--     </tr> -->
<!--   </tbody> -->
<!-- </table> -->

<!-- <br> -->
<!-- <br> -->
<!-- <table align="center" width="900px"> -->
<!--   <tbody> -->
<!--     <tr> -->
<!--       <td width="600px"> -->
<!--         <center> -->
<!--           <img src="./index_files/experiment_results.jpg" width="800px"><br> -->
<!--         </center> -->
<!--       </td> -->
<!--     </tr> -->
<!--     <tr><td width="600px"> -->
<!--         <center> -->
<!--           <span style="font-size:14px"><i> <span style="font-weight:bold">Metrics</span> In real-world experiments LB-WayPtNav-DH continues to perform well, however performance of both LB-WayPtNav and Mapping-SH degrades significantly as they do not take into account the <span style="font-weight:bold">dynamic</span> nature of the human. When Mapping-SH does succeed, it reaches the goal significantly faster than LBWayPtNav-DH and LB-WayPtNav, as it is able to exploit the precise geometry of the scene and barely avoid collision with the human. It does so, by executing a "last-minute" aggressive stopping manouvre to avoid collision, explaining the high jerk of Mapping-SH.</i> -->
<!--           </span> -->
<!--         </center> -->
<!--       </td> -->
<!--     </tr> -->
<!--   </tbody> -->
<!-- </table> -->


<!-- <br> -->
<!-- <center><h2>Videos</h2></center> -->
<!-- <table align="center"> -->
<!--   <tbody> -->
<!--     <tr> -->
<!--       <td> -->
<!--         <center> -->
<!--           <iframe width="600" height="338" src="./index_files/wYisoZBu1Y0.html" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br> -->
<!--         </center> -->
<!--       </td> -->
<!--     </tr> -->
<!--     <tr> -->
<!--       <td width="800px"> -->
<!--         <center> -->
<!--           <span style="font-size:14px"><i> <span style="font-weight:bold">Experiments 1: </span>The robot is tasked with moving 5 meters forward, while avoiding the human. Roughly halfway through its trajectory, the human decides to change direction, forcing the robot to react. LB-WayPtNav-DH reasons about the human's short-term future trajectory predicting a waypoint which ultimately avoids collision by moving to the right. LB-WayPtNav and Mapping-SH treat the human as a static obstacle, predict a waypoint to the left of the human (in its direction of motion) and ultimately collide.</i></span> -->
<!--           <br><br><br> -->
<!--         </center> -->
<!--       </td> -->
<!--     </tr> -->

<!--     <tr> -->
<!--       <td> -->
<!--         <center> -->
<!--           <iframe width="600" height="338" src="./index_files/mZ65ZgjI3yQ.html" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br> -->
<!--         </center> -->
<!--       </td> -->
<!--     </tr> -->
<!--   </tbody> -->
<!-- </table> -->

<!-- <table align="center"> -->
<!--   <tbody> -->
<!--     <tr> -->
<!--       <td width="800px"> -->
<!--         <center> -->
<!--           <span style="font-size:14px"><i> <span style="font-weight:bold">Experiment 2: </span>The robot is tasked with navigating to a goal down the hallway and around the corner, however there is also a human walking around the corner towards the robot. LB-WayPtNav-DH takes a more cautious trajectory around the corner, and thus is able to react and avoid the human. Mapping-SH and LB-WayPtNav both try to take aggressive trajectories around the corner guiding them almost head on into the human. When the do finally recognize the human they treat it as a static obstacle and ultimately collide. </i></span> -->
<!--           <br><br><br> -->
<!--         </center> -->
<!--       </td> -->
<!--     </tr> -->
<!--   </tbody> -->
<!-- </table> -->



